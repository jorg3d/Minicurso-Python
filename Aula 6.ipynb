{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Introdução\n",
    "\n",
    "Nesta última aula, iremos abordar o Topic Modeling, como último algoritmo não supervisionado para análise de dados textuais.Continuaremos com o mesmo conjunto de dados da BBC, para facilitar, porém com um estudo diferente sobre eles.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fundamentação Teórica\n",
    "\n",
    "Algoritmos de modelagem de tópicos são algoritmos frequentemente utilizados em processamento em grandes bases textuais à fim de obter informações, mostrando que um documento, diferente do cluster, pode pertencer a vários grupos de tópicos, sendo um tópico a distribuição de termos pertencentes à um único tema.(WANG & BLEI, 2011)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spacy\n",
    "\n",
    "Antes de iniciar a prática para implementar o código, vamos precisar instalar O Spacy, que é uma biblioteca para o Processamento de Linguagem Natural. Visto que vamos realizar o Topic Modeling, geralmente apenas os substantivos são necessários para gerar tópicos sobre uma base textual, sendo assim, com o spacy podemos realizar procedimentos sobre os textos utilizados a fim de filtrarmos somente os substantivos dos mesmos.\n",
    "\n",
    "Para instalar o Spacy no Anaconda, basta seguir os passos abaixo:\n",
    " - Abra o Anaconda Prompt no modo Administrador\n",
    " - Com o prompt aberto, digite \"conda install -c conda-forge spacy\"\n",
    " - Após isso, digite \"python -m spacy download en_core_web_sm\"\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementação do Código\n",
    "\n",
    "Com o spacy instalado, podemos agora iniciar a implementação do código, vamos começar com a importação das bibliotecas inicialmente necessárias, que serão o Pandas para manipular o data frame e o Spacy para processar os textos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Com o Spacy importado, criaremos uma instância dele para podermos processar os textos do data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pln = spacy.load(\"en_core_web_sm\", disable=['parser','ner'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Com a instância do spacy criado, podemos agora importar os dados, alocando-os em um data frame, para que possamos realizar o processamento destes dados. Dessa forma, iremos pegar cada notícia e retirarmos as palavras desnecessárias, ou seja, no nosso caso, vamos deixar apenas os substantivos presentes em cada notícia.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"noticias_bbc.csv\")\n",
    "\n",
    "def txt_substantivos(texto):\n",
    "    resultado = []\n",
    "    for doc in pln.pipe(texto):\n",
    "        subst_texto= \" \".join(token.lemma_ for token in doc if token.pos_=='NOUN')\n",
    "        resultado.append(subst_texto)\n",
    "    return resultado\n",
    "\n",
    "df['texto']=txt_substantivos(df['texto'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>texto</th>\n",
       "      <th>categoria</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>boss bag award executive business magazine tit...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>copy bumper sale fi shooter game copy sale com...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>msp climate warning climate change control dec...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>pavey success view week race track bronze inju...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>tory rethink association candidate election ag...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                              texto  categoria\n",
       "0           0  boss bag award executive business magazine tit...          0\n",
       "1           1  copy bumper sale fi shooter game copy sale com...          4\n",
       "2           2  msp climate warning climate change control dec...          2\n",
       "3           3  pavey success view week race track bronze inju...          3\n",
       "4           4  tory rethink association candidate election ag...          2"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Com os dados devidamente processados, podemos seguir para o próximo passo, que é estruturá-los em uma BoW aliado ao TF-IDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importando TF-IDF\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vec= TfidfVectorizer(max_features=5000, stop_words=\"english\", max_df=0.95,min_df=2)\n",
    "\n",
    "features = vec.fit_transform(df['texto'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dado isso, vamos criar a instância do modelo de Topic Modeling, que é o NMF (Non-Negative Matrix Factorization), e treiná-lo com nossas features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NMF(alpha=0.0, beta_loss='frobenius', init=None, l1_ratio=0.0, max_iter=200,\n",
       "    n_components=5, random_state=123, shuffle=False, solver='cd', tol=0.0001,\n",
       "    verbose=0)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Importando classificador NMF\n",
    "from sklearn.decomposition import NMF\n",
    "cls = NMF(n_components = 5, random_state=123)\n",
    "\n",
    "cls.fit(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Temos agora nosso modelo devidamente treinado, portanto, agora, precisamos extrair os resultados obtidos. Primeiramente vamos criar uma variável que contém o vetor com todas as 5000 palavras, para que possamos \"pegá-las\" através do índice no vetor mais tarde. Dessa forma, vamos criar a variável feature_names, como visto a seguir.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = vec.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como limitamos à 5000 mil palavras por vetor, vamos agora então limitar a quantidade de palavras que definem o nosso tópico, no nosso exemplo iremos fixar este valor em 15 palavras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_max_palavras = 15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora vamos à parte mais importante, que é de fato conseguir recuperar estas 15 palavras por tópico. Contudo vamos à uma breve explicação do código à seguir e das funções utilizadas nele.\n",
    "\n",
    "Primeiramente, com enumerate() conseguimos literalmente enumerar um conjunto de dados. No nosso caso, iremos enumerar cls.components_, que é onde estão os vetores de palavras de cada tópico, sendo assim, através de i conseguimos acessar o número da enumeração dos vetores e através de vet_topico acessar os vetores que foram gerados para cada tópico.\n",
    "\n",
    "Por fim, no segundo for, iremos aplicar o método argsort() em cada vetor dos tópicos. Dessa forma, com argsort() iremos reorganizar cada um desses vetores, fazendo com que o índices que contém  os menores valores (palavras menos significativas) se encontrem nos primeiros índices, enquanto que os índices das palavras que mais representam um tópico, ou seja, as mais significativas, sejam colocadas nos últimos índices. Portanto, aplicar o argsort iremos ordenar por ordem crescente de importâncias os índices dos termos de cada vetor.\n",
    "Sendo assim, com o fancy indexing realizado em \"[-1:-n_max_palavras-1:-1]\", iremos pegar os últimos 15 itens do vetor, ou seja, iremos pegar os valores dos índices que iremos utilizar para recuperar as palavras em feature_names, e, por fim, imprimi-los para podermos analisar os resultados.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 growth sale economy year company market share rate price firm profit oil analyst month quarter \n",
      "1 film award actor star actress director nomination movie year comedy role festival prize category ceremony \n",
      "2 game player match team injury club time win season coach goal victory title champion half \n",
      "3 election party government tax minister leader people campaign chancellor plan issue voter country taxis vote \n",
      "4 phone people music technology service user broadband software computer tv network device video site mobile \n"
     ]
    }
   ],
   "source": [
    "for i, vet_topico in enumerate(cls.components_):\n",
    "    print(i, end=\" \")\n",
    "    for id_palavra in vet_topico.argsort()[-1:-n_max_palavras-1:-1]:\n",
    "        print(feature_names[id_palavra], end=' ')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feito isso tudo, temos uma visão bastante significativa do resultado obtido, fazendo com que possamos aplicar novos dados para verificar em qual tópico eles se encaixam.\n",
    "\n",
    "Sendo assim, vamos criar uma variável com algumas strings, simulando um vetor com notícias em cada índice, para que seja mostrado um resultado para novos dados.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "novas_noticias = [\"Oil prices have weakened sharply because of a combination of oversupply and a collapse in global demand due to the decline in economic activity caused by coronavirus lockdown measures.\",\n",
    "                  \"The Haunting of Hill House premiered on the streamer back in 2018 to great success and critical acclaim. Last year, it was announced there would be a second season of the show, but with a completely different premise.\",\n",
    "                  \"It might be that boxing takes longer to get going again than other sports because of the reliance on medical support. What struck me was the last statement by the board on recognising how NHS staff would need time to rest and recuperate.\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A seguir, iremos pegar este novo conjunto de dados e aplicá-los ao nosso classificador."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 2, 2], dtype=int64)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cls.transform(vec.transform(novas_noticias)).argsort(axis=1)[:,-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Com isso vemos que 2 dos 3 textos foram classificados corretamente de acordo com o nosso modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "REFERENCIAS\n",
    "\n",
    "WANG, Chong; BLEI, David M. Collaborative topic modeling for recommending scientific articles. In: Proceedings of the 17th ACM SIGKDD international conference on Knowledge discovery and data mining. 2011. p. 448-456."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
